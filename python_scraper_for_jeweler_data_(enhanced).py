# -*- coding: utf-8 -*-
"""Python Scraper for Jeweler Data (Enhanced)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wfs9GfYDxmDLUNX9bLpt7jTK-TsHIYv-
"""

=# Install necessary libraries before running:
# pip install googlemaps pandas openpyxl

import googlemaps
import pandas as pd
import time

# --- CONFIGURATION ---
# IMPORTANT: Replace "YOUR_API_KEY" with your actual Google Maps Platform API key.
API_KEY = "AIzaSyDc9zWGUrObXLgIXJB103ERB7squIM89fg"

# List of cities to search.
CITIES = [
    "Anniston city, Alabama", "Bessemer city, Alabama", "Birmingham city, Alabama",
    "Gadsden city, Alabama", "Montgomery city, Alabama", "Pine Bluff city, Arkansas",
    "West Memphis city, Arkansas", "Barstow city, California", "Bell city, California",
    "Bell Gardens city, California", "Cudahy city, California", "Maywood city, California",
    "Selma city, California", "Bridgeport city, Connecticut", "Hartford city, Connecticut",
    "New Britain city, Connecticut", "New Haven city, Connecticut", "New London city, Connecticut",
    "Wilmington, Delaware", "Albany city, Georgia", "East Point city, Georgia",
    "Griffin city, Georgia", "Alton city, Illinois", "Calumet City city, Illinois",
    "Chicago Heights city, Illinois", "Danville city, Illinois", "Freeport city, Illinois",
    "Harvey city, Illinois", "Kankakee city, Illinois", "North Chicago city, Illinois",
    "Rock Island city, Illinois", "Rockford city, Illinois", "Anderson city, Indiana",
    "East Chicago city, Indiana", "Gary city, Indiana", "Hammond city, Indiana",
    "Marion city, Indiana", "Michigan City city, Indiana", "Muncie city, Indiana",
    "Richmond city, Indiana", "Terre Haute city, Indiana", "Ashland city, Kentucky",
    "Covington city, Kentucky", "Henderson city, Kentucky", "Hopkinsville city, Kentucky",
    "Owensboro city, Kentucky", "Paducah city, Kentucky", "Alexandria city, Louisiana",
    "Baton Rouge city, Louisiana", "Monroe city, Louisiana", "New Iberia city, Louisiana",
    "New Orleans city, Louisiana", "Shreveport city, Louisiana", "Baltimore city, Maryland",
    "Chelsea city, Massachusetts", "Holyoke city, Massachusetts", "Springfield city, Massachusetts",
    "Battle Creek city, Michigan", "Dearborn city, Michigan", "Dearborn Heights city, Michigan",
    "Detroit city, Michigan", "Flint city, Michigan", "Kalamazoo city, Michigan",
    "Lincoln Park city, Michigan", "Pontiac city, Michigan", "Saginaw city, Michigan",
    "Columbus city, Mississippi", "Greenville city, Mississippi", "Jackson city, Mississippi",
    "Meridian city, Mississippi", "Vicksburg city, Mississippi", "Atlantic City city, New Jersey",
    "Bridgeton city, New Jersey", "Camden city, New Jersey", "Newark city, New Jersey",
    "Passaic city, New Jersey", "Paterson city, New Jersey", "Perth Amboy city, New Jersey",
    "Trenton city, New Jersey", "Union City city, New Jersey", "Gallup city, New Mexico",
    "Auburn city, New York", "Binghamton city, New York", "Buffalo city, New York",
    "Elmira city, New York", "Jamestown city, New York", "Newburgh city, New York",
    "Niagara Falls city, New York", "Rochester city, New York", "Schenectady city, New York",
    "Syracuse city, New York", "Troy city, New York", "Utica city, New York",
    "Watertown city, New York", "Goldsboro city, North Carolina", "Wilson city, North Carolina",
    "Akron city, Ohio", "Alliance city, Ohio", "Canton city, Ohio",
    "Chillicothe city, Ohio", "Cincinnati city, Ohio", "Cleveland city, Ohio",
    "Dayton city, Ohio", "Elyria city, Ohio", "Euclid city, Ohio",
    "Garfield Heights city, Ohio", "Lima city, Ohio", "Lorain city, Ohio",
    "Mansfield city, Ohio", "Maple Heights city, Ohio", "Marion city, Ohio",
    "Middletown city, Ohio", "Sandusky city, Ohio", "Springfield city, Ohio",
    "Toledo city, Ohio", "Trotwood city, Ohio", "Warren city, Ohio",
    "Youngstown city, Ohio", "Zanesville city, Ohio", "Altoona city, Pennsylvania",
    "Chester city, Pennsylvania", "Erie city, Pennsylvania", "Harrisburg city, Pennsylvania",
    "Hazleton city, Pennsylvania", "Lebanon city, Pennsylvania", "New Castle city, Pennsylvania",
    "Philadelphia city, Pennsylvania", "Reading city, Pennsylvania", "Scranton city, Pennsylvania",
    "Wilkes-Barre city, Pennsylvania", "Williamsport city, Pennsylvania", "York city, Pennsylvania",
    "Central Falls, Rhode Island", "Providence, Rhode Island", "Anderson city, South Carolina",
    "Sumter city, South Carolina", "Beaumont city, Texas", "Kingsville city, Texas",
    "Lufkin city, Texas", "Marshall city, Texas", "Memphis city, Tennessee",
    "Nacogdoches city, Texas", "Port Arthur city, Texas", "Texarkana city, Texas",
    "Danville city, Virginia", "Hopewell city, Virginia", "Petersburg city, Virginia",
    "Huntington city, West Virginia", "Parkersburg city, West Virginia", "Milwaukee city, Wisconsin"
]

# --- NEW: List of different search queries to try for each city ---
# The {} will be replaced by the city name.
QUERY_VARIATIONS = [
    "jewelers in {}",
    "jewelry stores in {}",
    "diamond dealers in {}",
    "engagement rings in {}",
    "watch stores in {}",
    "goldsmiths in {}",
    "fine jewelry in {}"
]

# --- SCRIPT ---

def get_jeweler_details(api_key, cities):
    """
    Finds jewelers in a list of cities using multiple queries and saves them to an Excel file.
    """
    try:
        gmaps = googlemaps.Client(key=api_key)
        output_filename = "US_Jewelers.xlsx"

        with pd.ExcelWriter(output_filename, engine='openpyxl') as writer:
            for city in cities:
                print(f"--- Searching in {city} ---")

                # --- NEW: Use a set to store unique place IDs to avoid duplicates ---
                unique_place_ids = set()
                all_jewelers_for_city = []

                # --- NEW: Loop through each query variation ---
                for query_template in QUERY_VARIATIONS:
                    query = query_template.format(city)
                    print(f"  -> Using query: '{query}'")

                    try:
                        places_result = gmaps.places(query=query)
                    except googlemaps.exceptions.ApiError as e:
                        print(f"  ERROR: Could not perform search for query '{query}'. API Error: {e}")
                        continue

                    paginated_results = places_result.get('results', [])

                    next_page_token = places_result.get('next_page_token')
                    page_count = 1
                    while next_page_token and page_count < 3:
                        time.sleep(2)
                        try:
                            places_result = gmaps.places(query=query, page_token=next_page_token)
                            paginated_results.extend(places_result.get('results', []))
                            next_page_token = places_result.get('next_page_token')
                            page_count += 1
                        except googlemaps.exceptions.ApiError as e:
                            print(f"  ERROR: Could not fetch next page for query '{query}'. API Error: {e}")
                            break

                    # --- NEW: Process results and add only unique ones ---
                    for place in paginated_results:
                        place_id = place.get('place_id')
                        if place_id and place_id not in unique_place_ids:
                            unique_place_ids.add(place_id) # Add to set to mark as processed

                            try:
                                details = gmaps.place(place_id=place_id, fields=[
                                    'name', 'formatted_address', 'formatted_phone_number', 'website'
                                ])
                                result = details.get('result', {})
                                all_jewelers_for_city.append({
                                    'Name': result.get('name', 'N/A'),
                                    'Address': result.get('formatted_address', 'N/A'),
                                    'Phone Number': result.get('formatted_phone_number', 'N/A'),
                                    'Website': result.get('website', 'N/A'),
                                })
                            except googlemaps.exceptions.ApiError as e:
                                print(f"  ERROR: Could not get details for place_id {place_id}. API Error: {e}")

                if not all_jewelers_for_city:
                    print(f"No jewelers found for {city}.")
                    continue

                df = pd.DataFrame(all_jewelers_for_city)
                safe_sheet_name = "".join([c for c in city if c.isalpha() or c.isdigit() or c==' ']).rstrip()[:31]
                df.to_excel(writer, sheet_name=safe_sheet_name, index=False)
                print(f"Found a total of {len(all_jewelers_for_city)} unique jewelers in {city}. Saved to sheet: '{safe_sheet_name}'")

        print(f"\nâœ… Success! All data saved to {output_filename}")

    except Exception as e:
        print(f"An unexpected error occurred: {e}")

# --- RUN THE SCRIPT ---
if __name__ == "__main__":
    if API_KEY == "YOUR_API_KEY":
        print("ERROR: Please replace 'YOUR_API_KEY' with your actual Google Maps Platform API key.")
    else:
        get_jeweler_details(API_KEY, CITIES)





pip install googlemaps pandas openpyxl

import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import time
from typing import List, Dict
import os

class DiamondRetailerScraper:
    def __init__(self, api_key: str):
        """
        Initialize scraper with Google Maps API key.
        Get free API key from: https://console.cloud.google.com/
        """
        self.api_key = api_key
        self.base_url = "https://maps.googleapis.com/maps/api/place"

    def search_retailers(self, city: str, state: str, query: str = "diamond store") -> List[Dict]:
        """Search for diamond retailers in a specific city"""
        location = f"{city}, {state}, USA"

        # Text Search API endpoint
        search_url = f"{self.base_url}/textsearch/json"
        params = {
            'query': f"{query} in {location}",
            'key': self.api_key
        }

        try:
            response = requests.get(search_url, params=params)
            data = response.json()

            if data['status'] == 'OK':
                return data.get('results', [])
            else:
                print(f"Error for {city}, {state}: {data['status']}")
                return []
        except Exception as e:
            print(f"Exception for {city}, {state}: {str(e)}")
            return []

    def get_place_details(self, place_id: str) -> Dict:
        """Get detailed information about a place"""
        details_url = f"{self.base_url}/details/json"
        params = {
            'place_id': place_id,
            'fields': 'name,formatted_address,formatted_phone_number,website,business_status',
            'key': self.api_key
        }

        try:
            response = requests.get(details_url, params=params)
            data = response.json()

            if data['status'] == 'OK':
                return data.get('result', {})
            return {}
        except Exception as e:
            print(f"Error getting details: {str(e)}")
            return {}

    def extract_email_from_website(self, url: str) -> str:
        """Extract email from a website"""
        if not url:
            return ""

        try:
            # Add timeout and headers to avoid blocks
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')

            # Look for email patterns in text
            text = soup.get_text()
            email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
            emails = re.findall(email_pattern, text)

            # Filter out common non-contact emails
            filtered = [e for e in emails if not any(x in e.lower() for x in
                       ['example.com', 'sampleemail', 'youremail', 'wix.com', 'wordpress'])]

            return filtered[0] if filtered else ""
        except Exception as e:
            print(f"Error scraping {url}: {str(e)}")
            return ""

    def scrape_city(self, city: str, state: str) -> pd.DataFrame:
        """Scrape all diamond retailers in a city"""
        print(f"\nScraping {city}, {state}...")

        results = []
        retailers = self.search_retailers(city, state)

        for retailer in retailers:
            place_id = retailer.get('place_id')

            # Get detailed information
            details = self.get_place_details(place_id)
            time.sleep(0.5)  # Rate limiting

            name = details.get('name', retailer.get('name', ''))
            address = details.get('formatted_address', retailer.get('formatted_address', ''))
            phone = details.get('formatted_phone_number', '')
            website = details.get('website', '')

            # Extract email from website
            email = ""
            if website:
                email = self.extract_email_from_website(website)
                time.sleep(1)  # Be respectful with website scraping

            results.append({
                'Name': name,
                'Email': email,
                'Phone': phone,
                'Website': website,
                'Address': address,
                'City': city,
                'State': state
            })

            print(f"  Found: {name} | Email: {email or 'Not found'}")

        return pd.DataFrame(results)

    def scrape_by_state(self, cities_by_state: Dict[str, List[str]], output_file: str = "diamond_retailers_all_states.xlsx"):
        """
        Scrape all cities organized by state and save with state-wise sheets

        Args:
            cities_by_state: Dict like {'NY': ['New York', 'Buffalo'], 'CA': ['Los Angeles']}
            output_file: Name of output Excel file
        """
        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            all_data = []

            for state, cities in cities_by_state.items():
                print(f"\n{'='*50}")
                print(f"SCRAPING STATE: {state}")
                print(f"{'='*50}")

                state_data = []

                for city in cities:
                    df = self.scrape_city(city, state)

                    if not df.empty:
                        state_data.append(df)
                        all_data.append(df)

                    time.sleep(2)  # Rate limiting between cities

                # Save state sheet
                if state_data:
                    state_df = pd.concat(state_data, ignore_index=True)
                    state_df.to_excel(writer, sheet_name=state, index=False)
                    print(f"\nâœ“ {state}: {len(state_df)} retailers found")

            # Create summary sheet with all data
            if all_data:
                combined_df = pd.concat(all_data, ignore_index=True)
                combined_df.to_excel(writer, sheet_name='ALL_STATES', index=False)
                print(f"\n{'='*50}")
                print(f"âœ“ TOTAL: {len(combined_df)} retailers across all states")
                print(f"{'='*50}")

        print(f"\nâœ“ Data saved to {output_file}")
        return output_file


# ALL 50 STATES - Major Cities List
ALL_50_STATES_CITIES = {
    'AL': ['Birmingham', 'Montgomery', 'Mobile'],
    'AK': ['Anchorage', 'Fairbanks'],
    'AZ': ['Phoenix', 'Tucson', 'Scottsdale'],
    'AR': ['Little Rock', 'Fayetteville'],
    'CA': ['Los Angeles', 'San Francisco', 'San Diego', 'San Jose', 'Sacramento'],
    'CO': ['Denver', 'Colorado Springs', 'Boulder'],
    'CT': ['Hartford', 'New Haven', 'Stamford'],
    'DE': ['Wilmington', 'Dover'],
    'FL': ['Miami', 'Orlando', 'Tampa', 'Jacksonville', 'Fort Lauderdale'],
    'GA': ['Atlanta', 'Savannah', 'Augusta'],
    'HI': ['Honolulu', 'Hilo'],
    'ID': ['Boise', 'Idaho Falls'],
    'IL': ['Chicago', 'Springfield', 'Naperville'],
    'IN': ['Indianapolis', 'Fort Wayne', 'Carmel'],
    'IA': ['Des Moines', 'Cedar Rapids'],
    'KS': ['Wichita', 'Kansas City', 'Overland Park'],
    'KY': ['Louisville', 'Lexington'],
    'LA': ['New Orleans', 'Baton Rouge', 'Shreveport'],
    'ME': ['Portland', 'Bangor'],
    'MD': ['Baltimore', 'Annapolis', 'Rockville'],
    'MA': ['Boston', 'Worcester', 'Cambridge'],
    'MI': ['Detroit', 'Grand Rapids', 'Ann Arbor'],
    'MN': ['Minneapolis', 'Saint Paul', 'Rochester'],
    'MS': ['Jackson', 'Gulfport'],
    'MO': ['Kansas City', 'Saint Louis', 'Springfield'],
    'MT': ['Billings', 'Missoula'],
    'NE': ['Omaha', 'Lincoln'],
    'NV': ['Las Vegas', 'Reno', 'Henderson'],
    'NH': ['Manchester', 'Nashua'],
    'NJ': ['Newark', 'Jersey City', 'Princeton'],
    'NM': ['Albuquerque', 'Santa Fe'],
    'NY': ['New York', 'Buffalo', 'Rochester', 'Albany'],
    'NC': ['Charlotte', 'Raleigh', 'Greensboro'],
    'ND': ['Fargo', 'Bismarck'],
    'OH': ['Columbus', 'Cleveland', 'Cincinnati'],
    'OK': ['Oklahoma City', 'Tulsa'],
    'OR': ['Portland', 'Eugene', 'Salem'],
    'PA': ['Philadelphia', 'Pittsburgh', 'Allentown'],
    'RI': ['Providence', 'Warwick'],
    'SC': ['Charleston', 'Columbia', 'Greenville'],
    'SD': ['Sioux Falls', 'Rapid City'],
    'TN': ['Nashville', 'Memphis', 'Knoxville'],
    'TX': ['Houston', 'Dallas', 'Austin', 'San Antonio', 'Fort Worth'],
    'UT': ['Salt Lake City', 'Provo'],
    'VT': ['Burlington', 'Montpelier'],
    'VA': ['Virginia Beach', 'Richmond', 'Arlington'],
    'WA': ['Seattle', 'Spokane', 'Tacoma'],
    'WV': ['Charleston', 'Huntington'],
    'WI': ['Milwaukee', 'Madison', 'Green Bay'],
    'WY': ['Cheyenne', 'Casper']
}


# USAGE EXAMPLE
if __name__ == "__main__":
    # Step 1: Get your FREE Google Maps API key
    # Visit: https://console.cloud.google.com/google/maps-apis/start
    # Enable: Places API
    # You get $200 free credit per month (enough for 400+ cities)

    API_KEY = "AIzaSyDc9zWGUrObXLgIXJB103ERB7squIM89fg"  # Replace with your actual API key

    # Step 2: Initialize scraper
    scraper = DiamondRetailerScraper(API_KEY)

    # Step 3: Choose your scraping option

    # OPTION A: Scrape ALL 50 states (150+ cities)
    print("Starting scraping for ALL 50 STATES...")
    scraper.scrape_by_state(ALL_50_STATES_CITIES, "diamond_retailers_all_50_states.xlsx")

    # OPTION B: Scrape specific states only (uncomment to use)
    # specific_states = {
    #     'CA': ALL_50_STATES_CITIES['CA'],
    #     'NY': ALL_50_STATES_CITIES['NY'],
    #     'TX': ALL_50_STATES_CITIES['TX']
    # }
    # scraper.scrape_by_state(specific_states, "diamond_retailers_selected_states.xlsx")

    print("\n=== SCRAPING COMPLETE! ===")
    print("Excel file created with:")
    print("  - Individual sheet for each STATE")
    print("  - 'ALL_STATES' summary sheet")
    print("  - Name, Email, Phone, Website, Address for each retailer")